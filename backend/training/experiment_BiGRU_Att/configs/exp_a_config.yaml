# rehab-poc/backend/training/experiment_BiGRU_Att/configs/exp_a_config.yaml

# Configuration for Experiment A: BiGRU + Multihead-Attention

# --- DATA & PREPROCESSING SETTINGS ---
dataset:
  joints: 33          # MediaPipe 33 joints
  input_dim: 3          # 3D coordinates (x, y, z)
  window_size: 32       # T=32 frames
  stride: 8             # Overlap 75%
  
# --- MODEL ARCHITECTURE SETTINGS (Exp A Fixed) ---
model:
  encoder: bigru_attention
  frame_embed_dim: 128  # Output dimension of the initial MLP (FC(99) -> 128)
  
  bigru:
    layers: 2           # Fixed at 2 layers
    hidden: 256         # Hidden size per direction (Output: 512)
    dropout: 0.2
    
  attention:
    heads: 4            # Number of attention heads (Fixed)
    head_dim: 128       # Dimension per head
    
# --- HEADS (Output Regression) ---
heads:
  delta_angle_regression:
    output_dim: 6       # 6 angles (L/R Shoulder, L/R Hip, L/R Knee)
    hidden: 16
    
  log_variance_output:  # For L_uncertainty
    output_dim: 6       # Output log(sigma^2) for 6 angles
    hidden: 16

# --- LOSS WEIGHTS & SWEEP RANGE ---
loss:
  # The values to be inserted during the loop (will be overwritten by 'sweep' values)
  w_ang: 1.0
  w_unc: 0.0
  w_vel: 0.0
  
sweep:
  w_ang: [0.5, 1.0, 2.0]        # Weight for L_angle_MSE
  w_unc: [0.0, 0.05, 0.1, 0.2]  # Weight for L_uncertainty
  w_vel: [0.0, 0.01, 0.05, 0.1] # Weight for L_vel
  lr: [0.001, 0.0008, 0.0005]   # Learning Rate to sweep

# --- AUGMENTATION SETTINGS (Exp A Requirement) ---
# DataAug (baseline): basic affine (rot_z ±15°, rot_x/y ±5°), scale 0.9–1.1, jitter N(0,0.01), occlusion mask p=0.1.
augmentation:
  # Basic Affine Transformations
  basic:
    rot_z_deg: 15.0  # Rotation around Z-axis (Yaw)
    rot_xy_deg: 5.0  # Rotation around X/Y axes (Pitch/Roll) - (Currently not used in your _augment code but kept for completeness)
    scale_range: [0.9, 1.1] # Scaling factor range
    jitter_std: 0.01 # Standard deviation for Gaussian noise (Jitter)
  # Occlusion Masking
  occlusion:
    p_mask: 0.1  
    
# --- TRAINING & INFRA SETTINGS ---
training:
  epochs: 80
  batch_size: 32
  lr: 0.001 # Default LR (will be overwritten by sweep)
  weight_decay: 0.0001
  optimizer: AdamW
  early_stop_patience: 10

# Database connection details
database:
  host: "localhost"
  port: 5433
  dbname: "rehab_db"
  user: "nonny"
  password: "nonny"
  
logging:
  log_dir: "../results"
  log_file: "exp_a_loss_sweep.csv"